{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7196dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1/72 assets\r"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/Users/rentsher/Desktop/badri/scraping_test/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Replace this output directory with the directory you want to save the assets to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/rentsher/Desktop/badri/scraping_test/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mdownload_assets\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mdownload_assets\u001b[0;34m(url, output_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Save the asset to the appropriate file in the output directory\u001b[39;00m\n\u001b[1;32m     36\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     38\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(asset_response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Increment the asset download count and print the status\u001b[39;00m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/Users/rentsher/Desktop/badri/scraping_test/'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "def download_assets(url, output_dir):\n",
    "    # Create the output directory if it doesn't already exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Send a GET request to the URL and parse the HTML content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all of the script, image, and link tags in the HTML content\n",
    "    script_tags = soup.find_all('script')\n",
    "    img_tags = soup.find_all('img')\n",
    "    link_tags = soup.find_all('link')\n",
    "    total_assets = len(script_tags) + len(img_tags) + len(link_tags)\n",
    "    assets_downloaded = 0\n",
    "\n",
    "    # Download each asset and save it to the appropriate subdirectory\n",
    "    for tag in script_tags + img_tags + link_tags:\n",
    "        # Determine the asset URL and download it\n",
    "        asset_url = urljoin(url, tag.get('src') or tag.get('href'))\n",
    "        asset_response = requests.get(asset_url)\n",
    "\n",
    "        # Determine the path to the asset's output directory and create it if necessary\n",
    "        parsed_url = urlparse(asset_url)\n",
    "        path = parsed_url.path\n",
    "        if path.startswith('/'):\n",
    "            path = path[1:]\n",
    "        output_path = os.path.join(output_dir, os.path.dirname(path))\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Save the asset to the appropriate file in the output directory\n",
    "        filename = os.path.basename(path)\n",
    "        with open(os.path.join(output_path, filename), 'wb') as f:\n",
    "            f.write(asset_response.content)\n",
    "        \n",
    "        # Increment the asset download count and print the status\n",
    "        assets_downloaded += 1\n",
    "        print(f\"Downloaded {assets_downloaded}/{total_assets} assets\", end='\\r')\n",
    "        \n",
    "    print(\"\\nDone!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Replace this URL with the website you want to scrape\n",
    "    url = 'http://akkhi-techno-bright.com/'\n",
    "\n",
    "    # Replace this output directory with the directory you want to save the assets to\n",
    "    output_dir = '/Users/rentsher/Desktop/badri/scraping_test/'\n",
    "\n",
    "    download_assets(url, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4793518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing URL: https://yourstory.com/companies/search?page=1&sector=Marketplace&hitsPerPage=300Accessing URL: https://yourstory.com/companies/search?page=2&sector=Marketplace&hitsPerPage=300\n",
      "\n",
      "Accessing URL: https://yourstory.com/companies/search?page=3&sector=Marketplace&hitsPerPage=300\n",
      "Accessing URL: https://yourstory.com/companies/search?page=4&sector=Marketplace&hitsPerPage=300\n",
      "Accessing URL: https://yourstory.com/companies/search?page=5&sector=Marketplace&hitsPerPage=300\n",
      "Access Denied (403). Try using headers to mimic a legitimate user agent.Access Denied (403). Try using headers to mimic a legitimate user agent.\n",
      "Access Denied (403). Try using headers to mimic a legitimate user agent.\n",
      "Access Denied (403). Try using headers to mimic a legitimate user agent.Access Denied (403). Try using headers to mimic a legitimate user agent.\n",
      "\n",
      "\n",
      "Saving content to /Users/rentsher/Desktop/AllContentOutput.csv...\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import concurrent.futures\n",
    "\n",
    "def get_random_user_agent():\n",
    "    ua = UserAgent()\n",
    "    return ua.random\n",
    "\n",
    "def extract_content(url, headers=None):\n",
    "    try:\n",
    "        print(f\"Accessing URL: {url}\")\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all content from div elements with class 'sc-gEvEer gWlAOS'\n",
    "        print(\"Extracting content...\")\n",
    "        elements = soup.find_all('div', class_='sc-gEvEer gWlAOS')\n",
    "        extracted_content = [element.get_text(strip=True) for element in elements]\n",
    "\n",
    "        for i, content in enumerate(extracted_content, 1):\n",
    "            print(f\"Content #{i}: {content}\")\n",
    "\n",
    "        return extracted_content\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 403:\n",
    "            print(\"Access Denied (403). Try using headers to mimic a legitimate user agent.\")\n",
    "        else:\n",
    "            print(f\"HTTP Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(file_path, content):\n",
    "    with open(file_path, 'a', newline='', encoding='utf-8') as csvfile:  # Use 'a' for append mode\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerows([[item] for item in content])\n",
    "\n",
    "def process_content(output_csv_path):\n",
    "    base_url = \"https://yourstory.com/companies/search?page=\"\n",
    "    sector = \"&sector=Marketplace&hitsPerPage=300\"\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(extract_content, f\"{base_url}{i}{sector}\", headers={'User-Agent': get_random_user_agent()}) for i in range(1, 6)]  # Adjust the range as needed\n",
    "        concurrent.futures.wait(futures)\n",
    "        results = [future.result() for future in futures]\n",
    "\n",
    "    all_content = [item for sublist in results for item in sublist]\n",
    "\n",
    "    print(f\"Saving content to {output_csv_path}...\")\n",
    "    save_to_csv(output_csv_path, all_content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_csv_path = \"/Users/rentsher/Desktop/AllContentOutput.csv\"\n",
    "\n",
    "    # Truncate the existing output file or create a new one\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Content'])\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'User-Agent': get_random_user_agent()})\n",
    "\n",
    "    process_content(output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658658e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to obtain driver using Selenium Manager: Selenium Manager failed for: /Users/rentsher/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/common/macos/selenium-manager --browser chrome --output json.\n",
      "The chromedriver version cannot be discovered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Message: Selenium Manager failed for: /Users/rentsher/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/common/macos/selenium-manager --browser chrome --output json.\n",
      "The chromedriver version cannot be discovered\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "def get_image_links(domain_url, output_path='output_images.csv', min_image_size_kb=300):\n",
    "    driver = None\n",
    "\n",
    "    try:\n",
    "        # Set up Chrome options in headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        \n",
    "        # Initialize Chrome WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        # Fetch HTML content of the website using Selenium\n",
    "        driver.get(domain_url)\n",
    "\n",
    "        # Extract image links\n",
    "        image_links = []\n",
    "        for img_tag in driver.find_elements_by_tag_name('img'):\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "\n",
    "            # Fetch image data and check size\n",
    "            img_response = requests.get(img_url)\n",
    "            img_size_kb = len(img_response.content) / 1024\n",
    "\n",
    "            # If image size is not less than min_image_size_kb, add to the list\n",
    "            if img_size_kb >= min_image_size_kb:\n",
    "                image_links.append(img_url)\n",
    "                print(f'Image URL (Size: {img_size_kb} KB): {img_url}')\n",
    "\n",
    "        # Save the image URLs to CSV\n",
    "        with open(output_path, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerows([[url] for url in image_links])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "# Example usage\n",
    "domain_url = \"https://flipkart.com\"\n",
    "get_image_links(domain_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2477d28d4c962f377f35ef8e49a265e83d57fcfd4abeabec42f6362a97bde94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
